{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanaaria/Hyperparameter-Optimization/blob/main/Optimization_hyper_parameters_Esol.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IODlpprwyUlW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset\n",
        "from pandas import read_csv\n",
        "from sklearn.impute import SimpleImputer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "6FZvxwP-SPel",
        "outputId": "9de6794a-89b6-432a-cd35-c3b4256bd184"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1d64852b-3948-4f2e-8b94-4eddccb475c7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1d64852b-3948-4f2e-8b94-4eddccb475c7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Esol.csv to Esol.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        " \n",
        " \n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ebhdW5fqS7sh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import io\n",
        " \n",
        "data = pd.read_csv(io.BytesIO(uploaded['Esol.csv']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HKE89aPLaN7G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8ce3f15-1586-4038-8f85-2a8b99337eef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data.isnull().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MQv4GCtKcRis"
      },
      "outputs": [],
      "source": [
        "Xdata = data.iloc[:,0:200]\n",
        "Ydata = data.iloc[:,200:201]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "R1VP0adqcTrr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b6b8428-4b54-4853-e2a5-f030891f1450"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1128, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "Ydata.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zBeYZLA1cVX_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0f60d43-1068-410d-c0f5-f5f9a600fcc2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1128, 200)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "Xdata.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4sqBbEu-cXat"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(Xdata, Ydata, test_size=0.25, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SZHBTztdcZzA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50878dd8-d99e-446c-cacc-d81a44b48256"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(846, 200) (846, 1) (282, 200) (282, 1)\n"
          ]
        }
      ],
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "# scaler_data = preprocessing.MinMaxScaler()\n",
        "# X_train = scaler_data.fit_transform(X_train)\n",
        "# X_test = scaler_data.transform(X_test)\n",
        "scaler_labels = preprocessing.MinMaxScaler()\n",
        "Y_train = scaler_labels.fit_transform(Y_train.values.reshape(-1, 1))\n",
        "Y_test = scaler_labels.transform(Y_test.values.reshape(-1, 1))\n",
        "\n",
        "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)\n",
        "#print(X_train)\n",
        "#print(\"Train labels before scaling: {} {} {}Train labels after scaling: {} {}\".format('\\n',train_labels_before,'\\n', '\\n', train_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FDjzu0XGcdcZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26042b5a-c7be-4962-89bd-b732bb5f1c72"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "type(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "s5pNf3XRcfZm"
      },
      "outputs": [],
      "source": [
        "X_train=np.array(X_train)\n",
        "X_test=np.array(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SaFjyAxdwrWM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b941a80-2a12-4993-f74a-6b58035e70b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(846, 200)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tXxljbrgchsM"
      },
      "outputs": [],
      "source": [
        "# from sklearn.linear_model import LinearRegression\n",
        "# from sklearn import metrics\n",
        "# model= LinearRegression(fit_intercept=False)\n",
        "# model2=model.fit(X_train, Y_train)\n",
        "# print(model2.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "feE_qeLeckdD"
      },
      "outputs": [],
      "source": [
        "# from sklearn.decomposition import PCA as sklearnPCA\n",
        "# sklearn_pca = sklearnPCA(n_components=15)\n",
        "# X_train_pca = sklearn_pca.fit_transform(X_train)\n",
        "# print(X_train_pca.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-JwoRG2_cmLZ"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wUZVoY0rcnvl"
      },
      "outputs": [],
      "source": [
        "# transform to torch tensor\n",
        "\n",
        "tensor_x = torch.tensor(X_train, dtype=torch.float).to(device) \n",
        "tensor_x2 = torch.tensor(X_test, dtype=torch.float).to(device) \n",
        "\n",
        "tensor_y = torch.tensor(Y_train, dtype=torch.float).to(device)\n",
        "tensor_y2 = torch.tensor(Y_test, dtype=torch.float).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXmgtpZzcpbM",
        "outputId": "c12dfa91-a9f4-4967-933d-3f22118e8253"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "type(tensor_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "TXb77RA7crlQ"
      },
      "outputs": [],
      "source": [
        "trainset = TensorDataset(tensor_x, tensor_y) \n",
        "testset = TensorDataset(tensor_x2,tensor_y2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Q70hnYNLct9k"
      },
      "outputs": [],
      "source": [
        "def load_data(data_dir=None):\n",
        "    return trainset, testset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1c0XqNbZcv8r"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "try:\n",
        "    import ray\n",
        "except:\n",
        "    !pip install -U ray\n",
        "    import ray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Q8JiSDxpcx7g"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "import numpy as np\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split\n",
        "from torchsummary import summary\n",
        "\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ccVqHz8HoT-L"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "        self.hidden_dim1 = int(self.config.get(\"hidden_dim1\", 100))\n",
        "        self.hidden_dim2 = int(self.config.get(\"hidden_dim2\", 100))\n",
        "        self.hidden_dim3 = int(self.config.get(\"hidden_dim3\", 100))\n",
        "\n",
        "        self.act1 = self.config.get(\"act1\", \"relu\") \n",
        "        self.act2 = self.config.get(\"act2\", \"relu\") \n",
        "        self.act3 = self.config.get(\"act3\", \"relu\")\n",
        "\n",
        "        self.linear1 = nn.Linear(200, self.hidden_dim1)\n",
        "        self.linear2 = nn.Linear(self.hidden_dim1, self.hidden_dim2)\n",
        "        self.linear3 = nn.Linear(self.hidden_dim2, self.hidden_dim3)\n",
        "        self.linear4 = nn.Linear(self.hidden_dim3, 1)\n",
        "    \n",
        "    @staticmethod\n",
        "    def activation_func(act_str):\n",
        "        if act_str==\"tanh\":\n",
        "            return eval(\"torch.\"+act_str)\n",
        "        elif act_str==\"selu\" or act_str==\"relu\":   \n",
        "            return eval(\"torch.nn.functional.\"+act_str)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.linear1(x)\n",
        "        output = self.activation_func(self.act1)(output)\n",
        "        output = self.linear2(output)\n",
        "        output = self.activation_func(self.act2)(output)\n",
        "        output = self.linear3(output)\n",
        "        output = self.activation_func(self.act3)(output)\n",
        "        output = self.linear4(output)        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4_LpUtYWoYaG"
      },
      "outputs": [],
      "source": [
        "model = Net({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "IX6YOiTuoz63"
      },
      "outputs": [],
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad: continue\n",
        "        param = parameter.numel()\n",
        "        table.add_row([name, param])\n",
        "        total_params+=param\n",
        "    print(table)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFSQf_n4o1tP",
        "outputId": "6edb1f56-746c-4a48-c8df-014b4c4f002d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1               [-1, 1, 100]          20,100\n",
            "            Linear-2               [-1, 1, 100]          10,100\n",
            "            Linear-3               [-1, 1, 100]          10,100\n",
            "            Linear-4                 [-1, 1, 1]             101\n",
            "================================================================\n",
            "Total params: 40,401\n",
            "Trainable params: 40,401\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.15\n",
            "Estimated Total Size (MB): 0.16\n",
            "----------------------------------------------------------------\n",
            "+----------------+------------+\n",
            "|    Modules     | Parameters |\n",
            "+----------------+------------+\n",
            "| linear1.weight |   20000    |\n",
            "|  linear1.bias  |    100     |\n",
            "| linear2.weight |   10000    |\n",
            "|  linear2.bias  |    100     |\n",
            "| linear3.weight |   10000    |\n",
            "|  linear3.bias  |    100     |\n",
            "| linear4.weight |    100     |\n",
            "|  linear4.bias  |     1      |\n",
            "+----------------+------------+\n",
            "Total Trainable Params: 40401\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40401"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "summary(model, (1,tensor_x.shape[1]))\n",
        "\n",
        "count_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "4Ipyeowlo4UW"
      },
      "outputs": [],
      "source": [
        "def trainable_func(config, checkpoint_dir=None, data_dir=None, epochs=10):\n",
        "\n",
        "    net = Net(config)\n",
        "\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda:0\"\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            net = nn.DataParallel(net)\n",
        "    net.to(device)\n",
        "\n",
        "    '''\n",
        "    Define a loss function\n",
        "    '''\n",
        "    ## Classification\n",
        "    # criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    ## Regression\n",
        "    criterion = nn.MSELoss(reduction='sum')\n",
        "\n",
        "    # Define an optimizer \n",
        "    optimizer = optim.Adam(net.parameters(), lr=config.get(\"lr\",0.0003))\n",
        "\n",
        "    if checkpoint_dir:\n",
        "        model_state, optimizer_state = torch.load(\n",
        "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
        "        net.load_state_dict(model_state)\n",
        "        optimizer.load_state_dict(optimizer_state)\n",
        "\n",
        "    # Load data\n",
        "    trainset, testset = load_data(data_dir)\n",
        "\n",
        "    # Split the dataset into training and validation sets\n",
        "    train_size = int(len(trainset) * 0.66)\n",
        "    train_subset, val_subset = random_split(trainset, [train_size, len(trainset) - train_size])\n",
        "\n",
        "    # Define data loaders (which combines a dataset and a sampler, and provides an iterable over the given dataset)\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        train_subset,\n",
        "        batch_size=int(config.get(\"batch_size\",32)),\n",
        "        shuffle=True,\n",
        "        num_workers=2)\n",
        "    valloader = torch.utils.data.DataLoader(\n",
        "        val_subset,\n",
        "        batch_size=int(config.get(\"batch_size\",32)),\n",
        "        shuffle=True,\n",
        "        num_workers=2)\n",
        "\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "        epoch_train_loss = 0.0\n",
        "        # epoch_steps = 0\n",
        "        net.train() # Prepare model for training\n",
        "        for i, data in enumerate(trainloader):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            '''\n",
        "            Compute train loss without scaling to print\n",
        "            ''' \n",
        "            # outputs = torch.tensor(scaler_labels.inverse_transform(outputs.detach().cpu())).to(device)    \n",
        "            # labels = torch.tensor(scaler_labels.inverse_transform(labels.cpu())).to(device)  \n",
        "            # loss_train = criterion(outputs, labels) \n",
        "            # epoch_train_loss += loss_train.detach().item()\n",
        "        # print(\"[%d] loss: %.3f\" % (epoch + 1, epoch_train_loss / len(train_subset)))\n",
        "\n",
        "        # Validation loss\n",
        "        val_loss = 0.0\n",
        "        net.eval() # Prepare model for evaluation\n",
        "        for i, data in enumerate(valloader):\n",
        "            with torch.no_grad():\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = net(inputs)\n",
        "\n",
        "                # Inverse transform of the labels' scaler\n",
        "                outputs = torch.tensor(scaler_labels.inverse_transform(outputs.detach().cpu())).to(device)    \n",
        "                labels = torch.tensor(scaler_labels.inverse_transform(labels.cpu())).to(device) \n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.cpu().numpy()\n",
        "\n",
        "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
        "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
        "\n",
        "        tune.report(epoch = epoch, loss=(val_loss / len(val_subset)))\n",
        "    print(\"Finished Training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "DL9giDwao9Bk"
      },
      "outputs": [],
      "source": [
        "def test_score(config, net, device=\"cpu\"):\n",
        "    trainset, testset = load_data()\n",
        "\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "        testset, batch_size=int(config.get(\"batch_size\",32)), shuffle=False, num_workers=2)\n",
        "    \n",
        "    ## Regression\n",
        "    criterion = nn.MSELoss(reduction='sum')\n",
        "\n",
        "    # Test loss\n",
        "    test_loss = 0.0\n",
        "    net.eval() # Prepare model for evaluation\n",
        "    for i, data in enumerate(testloader):\n",
        "        with torch.no_grad():\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = net(inputs)\n",
        "\n",
        "            # Inverse transform of the labels' scaler\n",
        "            outputs = torch.tensor(scaler_labels.inverse_transform(outputs.detach().cpu())).to(device)    \n",
        "            labels = torch.tensor(scaler_labels.inverse_transform(labels.cpu())).to(device) \n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.cpu().numpy()\n",
        "\n",
        "    return test_loss / len(testset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "gI9cqZQHo_lT",
        "outputId": "60d22fc6-e2ab-49b4-8b6c-362821ffaac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'memory': 7892336640.0, 'CPU': 2.0, 'object_store_memory': 3946168320.0, 'node:172.28.0.2': 1.0}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "ray.shutdown()\n",
        "ray.init() # Here we use ray.init() to evaluate available_resources for Ray\n",
        "print(ray.available_resources())\n",
        "#ray.shutdown()  # Restart Ray defensively in case the ray connection is lost. \n",
        "\n",
        "# # Start Ray runtime with specific resources (not nessesarily all resources)\n",
        "# # You can change this values based on your machine resources)\n",
        "# ray.init(num_cpus=2, num_gpus=0) \n",
        "# print(ray.available_resources())\n",
        "# \"\"\"Check Ray Tune is working properly (for trainable class)\"\"\"\n",
        "# # from ray.tune.utils import validate_save_restore\n",
        "# # validate_save_restore(Trainable)\n",
        "# # validate_save_restore(Trainable, use_object_store=True)\n",
        "# # print(\"Success!\")\n",
        "\"\"\"\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "osC7wp3WpEBh"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "try:\n",
        "    import optuna\n",
        "except:\n",
        "    %pip install optuna\n",
        "    import optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install hpbandster ConfigSpace"
      ],
      "metadata": {
        "id": "RUZbWGLFJG3y"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "lQNGlLq0AxYV"
      },
      "outputs": [],
      "source": [
        "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
        "from ray.tune.suggest.optuna import OptunaSearch  \n",
        "from ray.tune.suggest.dragonfly import DragonflySearch\n",
        "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
        "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
        "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
        "from ray.tune.schedulers import MedianStoppingRule\n",
        "from ray.tune.schedulers import PopulationBasedTraining\n",
        "from ray.tune.suggest.bohb import TuneBOHB\n",
        "from ray.tune.suggest.basic_variant import BasicVariantGenerator\n",
        "from ray.tune.suggest import ConcurrencyLimiter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oFMVUl2ZpHL5",
        "outputId": "70eff956-09da-4ac2-f66b-3cd403395d50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-07-26 12:05:16,597\tINFO logger.py:630 -- pip install \"ray[tune]\" to see TensorBoard files.\n",
            "2022-07-26 12:05:16,598\tWARNING callback.py:106 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-26 12:05:20 (running for 00:00:03.56)<br>Memory usage on this node: 1.7/12.7 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
              "Bracket: Iter 8.000: None | Iter 4.000: None\n",
              "Bracket: Iter 8.000: None\n",
              "Bracket: \n",
              "Bracket: \n",
              "Bracket: <br>Resources requested: 0.5/2 CPUs, 0/0 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/trainable_func_2022-07-26_12-05-16<br>Number of trials: 1/10 (1 RUNNING)<br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial trainable_func_36483f98 reported epoch=0,loss=5.2032327999860515,should_checkpoint=True with parameters={'act1 ': 'relu', 'act2': 'tanh', 'act3': 'tanh', 'lr': 0.0007, 'batch_size': 32, 'hidden_dim1': 170.0, 'hidden_dim2': 190.0, 'hidden_dim3': 60.0}.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-26 12:05:28 (running for 00:00:11.94)<br>Memory usage on this node: 2.2/12.7 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
              "Bracket: Iter 8.000: None | Iter 4.000: None\n",
              "Bracket: Iter 8.000: None\n",
              "Bracket: \n",
              "Bracket: \n",
              "Bracket: <br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/trainable_func_2022-07-26_12-05-16<br>Number of trials: 5/10 (1 PENDING, 4 RUNNING)<br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial trainable_func_38629ed6 reported epoch=0,loss=6.918407483504482,should_checkpoint=True with parameters={'act1 ': 'selu', 'act2': 'tanh', 'act3': 'selu', 'lr': 0.0008, 'batch_size': 64, 'hidden_dim1': 190.0, 'hidden_dim2': 120.0, 'hidden_dim3': 120.0}.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-26 12:05:28 (running for 00:00:11.97)<br>Memory usage on this node: 2.2/12.7 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
              "Bracket: Iter 8.000: None | Iter 4.000: None\n",
              "Bracket: Iter 8.000: None\n",
              "Bracket: \n",
              "Bracket: \n",
              "Bracket: <br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/trainable_func_2022-07-26_12-05-16<br>Number of trials: 5/10 (1 PENDING, 4 RUNNING)<br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial trainable_func_386ac034 reported epoch=0,loss=12.009821741271498,should_checkpoint=True with parameters={'act1 ': 'relu', 'act2': 'selu', 'act3': 'tanh', 'lr': 0.0005, 'batch_size': 64, 'hidden_dim1': 90.0, 'hidden_dim2': 160.0, 'hidden_dim3': 120.0}.\n",
            "Trial trainable_func_38740162 reported epoch=0,loss=32.6003530643347,should_checkpoint=True with parameters={'act1 ': 'tanh', 'act2': 'relu', 'act3': 'relu', 'lr': 0.0008, 'batch_size': 128, 'hidden_dim1': 60.0, 'hidden_dim2': 110.0, 'hidden_dim3': 110.0}.\n",
            "Trial trainable_func_36483f98 reported epoch=4,loss=0.6425042386894011,should_checkpoint=True with parameters={'act1 ': 'relu', 'act2': 'tanh', 'act3': 'tanh', 'lr': 0.0007, 'batch_size': 32, 'hidden_dim1': 170.0, 'hidden_dim2': 190.0, 'hidden_dim3': 60.0}.\n",
            "Trial trainable_func_38740162 reported epoch=9,loss=1.6131215725473678,should_checkpoint=True with parameters={'act1 ': 'tanh', 'act2': 'relu', 'act3': 'relu', 'lr': 0.0008, 'batch_size': 128, 'hidden_dim1': 60.0, 'hidden_dim2': 110.0, 'hidden_dim3': 110.0}. This trial completed.\n",
            "Trial trainable_func_386ac034 reported epoch=9,loss=0.8937865429396239,should_checkpoint=True with parameters={'act1 ': 'relu', 'act2': 'selu', 'act3': 'tanh', 'lr': 0.0005, 'batch_size': 64, 'hidden_dim1': 90.0, 'hidden_dim2': 160.0, 'hidden_dim3': 120.0}. This trial completed.\n",
            "Trial trainable_func_36483f98 reported epoch=9,loss=0.5091429549302352,should_checkpoint=True with parameters={'act1 ': 'relu', 'act2': 'tanh', 'act3': 'tanh', 'lr': 0.0007, 'batch_size': 32, 'hidden_dim1': 170.0, 'hidden_dim2': 190.0, 'hidden_dim3': 60.0}. This trial completed.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-26 12:05:33 (running for 00:00:17.05)<br>Memory usage on this node: 2.2/12.7 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
              "Bracket: Iter 8.000: -0.7881401878117209 | Iter 4.000: -1.962813541984698\n",
              "Bracket: Iter 8.000: None\n",
              "Bracket: \n",
              "Bracket: \n",
              "Bracket: <br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/trainable_func_2022-07-26_12-05-16<br>Number of trials: 7/10 (1 PENDING, 4 RUNNING, 2 TERMINATED)<br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial trainable_func_38629ed6 reported epoch=9,loss=0.4800202615008023,should_checkpoint=True with parameters={'act1 ': 'selu', 'act2': 'tanh', 'act3': 'selu', 'lr': 0.0008, 'batch_size': 64, 'hidden_dim1': 190.0, 'hidden_dim2': 120.0, 'hidden_dim3': 120.0}. This trial completed.\n",
            "Trial trainable_func_3880fac0 reported epoch=0,loss=9.126491238255639,should_checkpoint=True with parameters={'act1 ': 'tanh', 'act2': 'relu', 'act3': 'relu', 'lr': 0.0007, 'batch_size': 32, 'hidden_dim1': 100.0, 'hidden_dim2': 130.0, 'hidden_dim3': 50.0}.\n",
            "Trial trainable_func_401a6e92 reported epoch=0,loss=5.984780611784498,should_checkpoint=True with parameters={'act1 ': 'tanh', 'act2': 'selu', 'act3': 'selu', 'lr': 0.0007, 'batch_size': 128, 'hidden_dim1': 60.0, 'hidden_dim2': 150.0, 'hidden_dim3': 80.0}.\n",
            "Trial trainable_func_404e38a8 reported epoch=0,loss=2.3075031063385665,should_checkpoint=True with parameters={'act1 ': 'relu', 'act2': 'selu', 'act3': 'selu', 'lr': 0.0006000000000000001, 'batch_size': 32, 'hidden_dim1': 160.0, 'hidden_dim2': 150.0, 'hidden_dim3': 180.0}.\n",
            "Trial trainable_func_4073ef26 reported epoch=0,loss=1.667159775930249,should_checkpoint=True with parameters={'act1 ': 'relu', 'act2': 'tanh', 'act3': 'selu', 'lr': 0.001, 'batch_size': 16, 'hidden_dim1': 90.0, 'hidden_dim2': 190.0, 'hidden_dim3': 150.0}.\n",
            "Trial trainable_func_401a6e92 reported epoch=3,loss=4.971648100612621,should_checkpoint=True with parameters={'act1 ': 'tanh', 'act2': 'selu', 'act3': 'selu', 'lr': 0.0007, 'batch_size': 128, 'hidden_dim1': 60.0, 'hidden_dim2': 150.0, 'hidden_dim3': 80.0}. This trial completed.\n",
            "Trial trainable_func_408c90ee reported epoch=0,loss=4.81913782084685,should_checkpoint=True with parameters={'act1 ': 'relu', 'act2': 'tanh', 'act3': 'relu', 'lr': 0.0008, 'batch_size': 16, 'hidden_dim1': 80.0, 'hidden_dim2': 100.0, 'hidden_dim3': 160.0}.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-26 12:05:38 (running for 00:00:22.20)<br>Memory usage on this node: 2.2/12.7 GiB<br>Using AsyncHyperBand: num_stopped=1\n",
              "Bracket: Iter 8.000: -0.7881401878117209 | Iter 4.000: -1.962813541984698\n",
              "Bracket: Iter 8.000: None\n",
              "Bracket: \n",
              "Bracket: \n",
              "Bracket: <br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/trainable_func_2022-07-26_12-05-16<br>Number of trials: 10/10 (1 PENDING, 4 RUNNING, 5 TERMINATED)<br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial trainable_func_3880fac0 reported epoch=8,loss=0.8529000645494234,should_checkpoint=True with parameters={'act1 ': 'tanh', 'act2': 'relu', 'act3': 'relu', 'lr': 0.0007, 'batch_size': 32, 'hidden_dim1': 100.0, 'hidden_dim2': 130.0, 'hidden_dim3': 50.0}.\n",
            "Trial trainable_func_404e38a8 reported epoch=7,loss=0.611577644849814,should_checkpoint=True with parameters={'act1 ': 'relu', 'act2': 'selu', 'act3': 'selu', 'lr': 0.0006000000000000001, 'batch_size': 32, 'hidden_dim1': 160.0, 'hidden_dim2': 150.0, 'hidden_dim3': 180.0}.\n",
            "Trial trainable_func_3880fac0 reported epoch=9,loss=1.0848940464894796,should_checkpoint=True with parameters={'act1 ': 'tanh', 'act2': 'relu', 'act3': 'relu', 'lr': 0.0007, 'batch_size': 32, 'hidden_dim1': 100.0, 'hidden_dim2': 130.0, 'hidden_dim3': 50.0}. This trial completed.\n",
            "Trial trainable_func_41767c5e reported epoch=0,loss=10.072503665910043,should_checkpoint=True with parameters={'act1 ': 'relu', 'act2': 'relu', 'act3': 'selu', 'lr': 0.001, 'batch_size': 128, 'hidden_dim1': 80.0, 'hidden_dim2': 50.0, 'hidden_dim3': 110.0}.\n",
            "Trial trainable_func_4073ef26 reported epoch=5,loss=0.613038660707288,should_checkpoint=True with parameters={'act1 ': 'relu', 'act2': 'tanh', 'act3': 'selu', 'lr': 0.001, 'batch_size': 16, 'hidden_dim1': 90.0, 'hidden_dim2': 190.0, 'hidden_dim3': 150.0}.\n",
            "Trial trainable_func_404e38a8 reported epoch=9,loss=0.5694354939149585,should_checkpoint=True with parameters={'act1 ': 'relu', 'act2': 'selu', 'act3': 'selu', 'lr': 0.0006000000000000001, 'batch_size': 32, 'hidden_dim1': 160.0, 'hidden_dim2': 150.0, 'hidden_dim3': 180.0}. This trial completed.\n",
            "Trial trainable_func_408c90ee reported epoch=5,loss=0.672507589147837,should_checkpoint=True with parameters={'act1 ': 'relu', 'act2': 'tanh', 'act3': 'relu', 'lr': 0.0008, 'batch_size': 16, 'hidden_dim1': 80.0, 'hidden_dim2': 100.0, 'hidden_dim3': 160.0}.\n",
            "Trial trainable_func_41767c5e reported epoch=3,loss=5.409156866967525,should_checkpoint=True with parameters={'act1 ': 'relu', 'act2': 'relu', 'act3': 'selu', 'lr': 0.001, 'batch_size': 128, 'hidden_dim1': 80.0, 'hidden_dim2': 50.0, 'hidden_dim3': 110.0}. This trial completed.\n",
            "Trial trainable_func_408c90ee reported epoch=9,loss=0.5341926762448956,should_checkpoint=True with parameters={'act1 ': 'relu', 'act2': 'tanh', 'act3': 'relu', 'lr': 0.0008, 'batch_size': 16, 'hidden_dim1': 80.0, 'hidden_dim2': 100.0, 'hidden_dim3': 160.0}. This trial completed.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-26 12:05:44 (running for 00:00:27.85)<br>Memory usage on this node: 2.1/12.7 GiB<br>Using AsyncHyperBand: num_stopped=2\n",
              "Bracket: Iter 8.000: -0.6042530714890477 | Iter 4.000: -1.962813541984698\n",
              "Bracket: Iter 8.000: None\n",
              "Bracket: \n",
              "Bracket: \n",
              "Bracket: <br>Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/trainable_func_2022-07-26_12-05-16<br>Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)<br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial trainable_func_4073ef26 reported epoch=9,loss=0.4359037326724615,should_checkpoint=True with parameters={'act1 ': 'relu', 'act2': 'tanh', 'act3': 'selu', 'lr': 0.001, 'batch_size': 16, 'hidden_dim1': 90.0, 'hidden_dim2': 190.0, 'hidden_dim3': 150.0}. This trial completed.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-26 12:05:44 (running for 00:00:27.96)<br>Memory usage on this node: 1.8/12.7 GiB<br>Using AsyncHyperBand: num_stopped=2\n",
              "Bracket: Iter 8.000: -0.6042530714890477 | Iter 4.000: -1.962813541984698\n",
              "Bracket: Iter 8.000: None\n",
              "Bracket: \n",
              "Bracket: \n",
              "Bracket: <br>Resources requested: 0/2 CPUs, 0/0 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/trainable_func_2022-07-26_12-05-16<br>Number of trials: 10/10 (10 TERMINATED)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name             </th><th>status    </th><th>loc           </th><th>act1   </th><th>act2  </th><th>act3  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  hidden_dim1</th><th style=\"text-align: right;\">  hidden_dim2</th><th style=\"text-align: right;\">  hidden_dim3</th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  epoch</th><th style=\"text-align: right;\">    loss</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>trainable_func_36483f98</td><td>TERMINATED</td><td>172.28.0.2:551</td><td>relu   </td><td>tanh  </td><td>tanh  </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          170</td><td style=\"text-align: right;\">          190</td><td style=\"text-align: right;\">           60</td><td style=\"text-align: right;\">0.0007</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        13.4785 </td><td style=\"text-align: right;\">      9</td><td style=\"text-align: right;\">0.509143</td></tr>\n",
              "<tr><td>trainable_func_38629ed6</td><td>TERMINATED</td><td>172.28.0.2:591</td><td>selu   </td><td>tanh  </td><td>selu  </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          190</td><td style=\"text-align: right;\">          120</td><td style=\"text-align: right;\">          120</td><td style=\"text-align: right;\">0.0008</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         6.81636</td><td style=\"text-align: right;\">      9</td><td style=\"text-align: right;\">0.48002 </td></tr>\n",
              "<tr><td>trainable_func_386ac034</td><td>TERMINATED</td><td>172.28.0.2:596</td><td>relu   </td><td>selu  </td><td>tanh  </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           90</td><td style=\"text-align: right;\">          160</td><td style=\"text-align: right;\">          120</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         5.8261 </td><td style=\"text-align: right;\">      9</td><td style=\"text-align: right;\">0.893787</td></tr>\n",
              "<tr><td>trainable_func_38740162</td><td>TERMINATED</td><td>172.28.0.2:703</td><td>tanh   </td><td>relu  </td><td>relu  </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           60</td><td style=\"text-align: right;\">          110</td><td style=\"text-align: right;\">          110</td><td style=\"text-align: right;\">0.0008</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         4.54516</td><td style=\"text-align: right;\">      9</td><td style=\"text-align: right;\">1.61312 </td></tr>\n",
              "<tr><td>trainable_func_3880fac0</td><td>TERMINATED</td><td>172.28.0.2:703</td><td>tanh   </td><td>relu  </td><td>relu  </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">          130</td><td style=\"text-align: right;\">           50</td><td style=\"text-align: right;\">0.0007</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         7.2511 </td><td style=\"text-align: right;\">      9</td><td style=\"text-align: right;\">1.08489 </td></tr>\n",
              "<tr><td>trainable_func_401a6e92</td><td>TERMINATED</td><td>172.28.0.2:596</td><td>tanh   </td><td>selu  </td><td>selu  </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           60</td><td style=\"text-align: right;\">          150</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">0.0007</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         1.90311</td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">4.97165 </td></tr>\n",
              "<tr><td>trainable_func_404e38a8</td><td>TERMINATED</td><td>172.28.0.2:551</td><td>relu   </td><td>selu  </td><td>selu  </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          160</td><td style=\"text-align: right;\">          150</td><td style=\"text-align: right;\">          180</td><td style=\"text-align: right;\">0.0006</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         7.84387</td><td style=\"text-align: right;\">      9</td><td style=\"text-align: right;\">0.569435</td></tr>\n",
              "<tr><td>trainable_func_4073ef26</td><td>TERMINATED</td><td>172.28.0.2:591</td><td>relu   </td><td>tanh  </td><td>selu  </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           90</td><td style=\"text-align: right;\">          190</td><td style=\"text-align: right;\">          150</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        10.589  </td><td style=\"text-align: right;\">      9</td><td style=\"text-align: right;\">0.435904</td></tr>\n",
              "<tr><td>trainable_func_408c90ee</td><td>TERMINATED</td><td>172.28.0.2:596</td><td>relu   </td><td>tanh  </td><td>relu  </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">          160</td><td style=\"text-align: right;\">0.0008</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         9.04729</td><td style=\"text-align: right;\">      9</td><td style=\"text-align: right;\">0.534193</td></tr>\n",
              "<tr><td>trainable_func_41767c5e</td><td>TERMINATED</td><td>172.28.0.2:703</td><td>relu   </td><td>relu  </td><td>selu  </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">           50</td><td style=\"text-align: right;\">          110</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         1.63103</td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">5.40916 </td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-07-26 12:05:44,819\tINFO tune.py:748 -- Total run time: 28.26 seconds (27.89 seconds for the tuning loop).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial config: {'act1 ': 'relu', 'act2': 'tanh', 'act3': 'selu', 'lr': 0.001, 'batch_size': 16, 'hidden_dim1': 90.0, 'hidden_dim2': 190.0, 'hidden_dim3': 150.0}\n",
            "Best trial final validation score: 0.4359037326724615\n",
            "Best trial test set score: 0.45816472594560614\n"
          ]
        }
      ],
      "source": [
        "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n",
        "\n",
        "    # define data directory here if you want to load data from files\n",
        "    data_dir = os.path.abspath(\"./Esol\")\n",
        "    load_data()\n",
        "\n",
        "    # define the search space of hyperparameters\n",
        "    config = {\n",
        "        \"act1 \": tune.choice([\"relu\",\"tanh\",\"selu\"]),\n",
        "        \"act2\" : tune.choice([\"relu\",\"tanh\",\"selu\"]),\n",
        "        \"act3\" : tune.choice([\"relu\",\"tanh\",\"selu\"]),\n",
        "        \"lr\": tune.quniform(0.0005, 0.001, 0.0001),\n",
        "        \"batch_size\": tune.choice([8, 16, 32 ,64,128]),\n",
        "        \"hidden_dim1\" : tune.quniform(50, 200, 10),\n",
        "        \"hidden_dim2\" : tune.quniform(50, 200, 10),\n",
        "        \"hidden_dim3\" : tune.quniform(50, 200, 10),\n",
        "    }\n",
        "\n",
        "    # Optuna search algorithm\n",
        "    from ray.tune.suggest.optuna import OptunaSearch \n",
        "    from ray.tune.suggest import ConcurrencyLimiter\n",
        "    search_alg = OptunaSearch(\n",
        "        metric=\"loss\", #or accuracy, etc.\n",
        "        mode=\"min\", #or max\n",
        "        # seed = 42,\n",
        "        # points_to_evaluate=[\n",
        "        # {'lr': 0.0005, 'hidden_size': 150.0, 'readout1_out': 200.0, 'readout2_out': 180.0}\n",
        "        # ],\n",
        "        )\n",
        "    \n",
        "    search_alg = ConcurrencyLimiter(search_alg, max_concurrent=10)\n",
        "\n",
        "    scheduler = ASHAScheduler(\n",
        "        metric =\"loss\",\n",
        "        mode=\"min\",\n",
        "        max_t=max_num_epochs,\n",
        "        reduction_factor=2, \n",
        "        grace_period=4,\n",
        "        brackets=5\n",
        "        )\n",
        "    \n",
        "   \n",
        "    # wrap data loading and training for tuning using `partial` \n",
        "    # (note that there exist other methods for this purpose)\n",
        "    result = tune.run(\n",
        "        partial(trainable_func, data_dir=data_dir, epochs=max_num_epochs),\n",
        "        scheduler=scheduler,\n",
        "        search_alg=search_alg,\n",
        "        num_samples=num_samples,\n",
        "        config=config,\n",
        "        verbose=2,\n",
        "        checkpoint_score_attr=\"loss\",\n",
        "        checkpoint_freq=0,\n",
        "        keep_checkpoints_num=1,\n",
        "        # checkpoint_at_end=True,\n",
        "        # reuse_actors=reuse_actors_status,\n",
        "        #progress_reporter=reporter,\n",
        "        resources_per_trial={\"cpu\": 0.5, \"gpu\": gpus_per_trial},\n",
        "        stop={\"training_iteration\": max_num_epochs},                \n",
        "        )\n",
        "\n",
        "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
        "    print(\"Best trial config: {}\".format(best_trial.config))\n",
        "    print(\"Best trial final validation score: {}\".format(\n",
        "        best_trial.last_result[\"loss\"]))\n",
        "\n",
        "    best_trained_model = Net(best_trial.config)\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda:0\"\n",
        "        if gpus_per_trial > 1:\n",
        "            best_trained_model = nn.DataParallel(best_trained_model)\n",
        "    best_trained_model.to(device)\n",
        "\n",
        "    best_checkpoint_dir = best_trial.checkpoint.value\n",
        "    model_state, optimizer_state = torch.load(os.path.join(\n",
        "        best_checkpoint_dir, \"checkpoint\"))\n",
        "    best_trained_model.load_state_dict(model_state)\n",
        "\n",
        "    test_score_value = test_score(best_trial.config, best_trained_model, device)\n",
        "    print(\"Best trial test set score: {}\".format(test_score_value))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # You can change the number of GPUs per trial here:\n",
        "    main(num_samples=10, max_num_epochs=10, gpus_per_trial=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4d5E9VULh6yy"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "576755a4-d8e0-43a6-e104-46b64ba484b1",
        "id": "OtPhxD_OBln3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Training\n",
            "0.3979180043482341\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "random.seed(1)\n",
        "Best_trial_config = {'act1 ': 'relu', 'act2': 'tanh', 'act3': 'selu', 'lr': 0.001, 'batch_size': 16, 'hidden_dim1': 90.0, 'hidden_dim2': 190.0, 'hidden_dim3': 150.0}\n",
        "epochs = 10\n",
        "config = {'act1 ': 'relu', 'act2': 'tanh', 'act3': 'selu', 'lr': 0.001, 'batch_size': 16, 'hidden_dim1': 90.0, 'hidden_dim2': 190.0, 'hidden_dim3': 150.0}\n",
        "net = Net(Best_trial_config)\n",
        "criterion = nn.MSELoss(reduction='sum')\n",
        "\n",
        "    # # Define an optimizer \n",
        "optimizer = optim.Adam(net.parameters(), lr=config.get(\"lr\",0.0003))\n",
        "\n",
        "trainset, testset = load_data()\n",
        "\n",
        "    # Split the dataset into training and validation sets\n",
        "train_size = int(len(trainset) * 0.8)\n",
        "train_subset, val_subset = random_split(trainset, [train_size, len(trainset) - train_size])\n",
        "\n",
        "    # Define data loaders (which combines a dataset and a sampler, and provides an iterable over the given dataset)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    train_subset,\n",
        "    batch_size=int(config.get(\"batch_size\",16)),\n",
        "    shuffle=True,\n",
        "    num_workers=2)\n",
        "valloader = torch.utils.data.DataLoader(\n",
        "    val_subset,\n",
        "    batch_size=int(config.get(\"batch_size\",16)),\n",
        "    shuffle=True,\n",
        "    num_workers=2)\n",
        "\n",
        "for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "     epoch_train_loss = 0.0\n",
        "        # epoch_steps = 0\n",
        "     net.train() # Prepare model for training\n",
        "     for i, data in enumerate(trainloader):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            '''\n",
        "            Compute train loss without scaling to print\n",
        "            ''' \n",
        "            # outputs = torch.tensor(scaler_labels.inverse_transform(outputs.detach().cpu())).to(device)    \n",
        "            # labels = torch.tensor(scaler_labels.inverse_transform(labels.cpu())).to(device)  \n",
        "            # loss_train = criterion(outputs, labels) \n",
        "            # epoch_train_loss += loss_train.detach().item()\n",
        "        # print(\"[%d] loss: %.3f\" % (epoch + 1, epoch_train_loss / len(train_subset)))\n",
        "\n",
        "        # Validation loss\n",
        "     val_loss = 0.0\n",
        "     net.eval() # Prepare model for evaluation\n",
        "     for i, data in enumerate(valloader):\n",
        "            with torch.no_grad():\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = net(inputs)\n",
        "\n",
        "                # Inverse transform of the labels' scaler\n",
        "                outputs = torch.tensor(scaler_labels.inverse_transform(outputs.detach().cpu())).to(device)    \n",
        "                labels = torch.tensor(scaler_labels.inverse_transform(labels.cpu())).to(device) \n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.cpu().numpy()\n",
        "\n",
        "        # with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
        "        #     path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "        #     torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
        "\n",
        "        # tune.report(epoch = epoch, loss=(val_loss / len(val_subset)))\n",
        "print(\"Finished Training\")\n",
        "device = \"cpu\"\n",
        "# best_trained_model.to(device)\n",
        "test_score_value = test_score(Best_trial_config, net, device)\n",
        "print(test_score_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-thYQk6c2U5"
      },
      "outputs": [],
      "source": [
        "# !pip install -U hyperopt\n",
        "# !pip install hpbandster ConfigSpace\n",
        "# !pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Na9ImKcRpKaV"
      },
      "outputs": [],
      "source": [
        "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
        "from ray.tune.suggest.optuna import OptunaSearch  \n",
        "from ray.tune.suggest.dragonfly import DragonflySearch\n",
        "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
        "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
        "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
        "from ray.tune.schedulers import MedianStoppingRule\n",
        "from ray.tune.schedulers import PopulationBasedTraining\n",
        "from ray.tune.suggest.bohb import TuneBOHB\n",
        "from ray.tune.suggest.basic_variant import BasicVariantGenerator\n",
        "from ray.tune.suggest import ConcurrencyLimiter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTFMuWRzpOHa"
      },
      "outputs": [],
      "source": [
        "training_iteration = 50 #max_num_epochs\n",
        "\n",
        "## ASHA\n",
        "scheduler = AsyncHyperBandScheduler(\n",
        "    time_attr=\"training_iteration\",\n",
        "    max_t=training_iteration,\n",
        "    metric=\"loss\", #or accuracy, etc.\n",
        "    mode=\"min\", #or max\n",
        "    reduction_factor=2, \n",
        "    grace_period=4,\n",
        "    brackets=5,\n",
        "    )\n",
        "\n",
        "## BOHB\n",
        "scheduler = HyperBandForBOHB(\n",
        "    time_attr=\"training_iteration\",\n",
        "    max_t=training_iteration, \n",
        "    reduction_factor=8, \n",
        "    stop_last_trials=True,\n",
        "    metric=\"loss\", #or accuracy, etc.\n",
        "    mode=\"min\", #or max\n",
        "    )\n",
        "\n",
        "## Median\n",
        "scheduler = MedianStoppingRule(\n",
        "    time_attr=\"training_iteration\",\n",
        "    grace_period=10,\n",
        "    min_samples_required=10,\n",
        "    hard_stop = True,\n",
        "    metric=\"loss\", #or accuracy, etc.\n",
        "    mode=\"min\", #or max\n",
        "    )\n",
        "\n",
        "## PBT\n",
        "scheduler = PopulationBasedTraining(\n",
        "    time_attr=\"training_iteration\",\n",
        "    metric=\"loss\", #or accuracy, etc.\n",
        "    mode=\"min\", #or max\n",
        "    perturbation_interval=10,  # every 10 `time_attr` units\n",
        "                            # (training_iterations in this case)\n",
        "    # hyperparam_mutations={\n",
        "    #     \"lr\": [8e-3, 7e-3, 6e-3, 5e-3, 4e-3],\n",
        "    #     \"dropout\": tune.quniform(0.0, 0.4, 0.05),\n",
        "    #     \"dropout1\": tune.quniform(0.0, 0.4, 0.05),\n",
        "    #     \"dropout2\": tune.quniform(0.0, 0.4, 0.05),\n",
        "    #     \"max_norm_val\":tune.choice([2.5, 3, 3.5, 4]),\n",
        "    #     }\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTEOu4x_1di3"
      },
      "outputs": [],
      "source": [
        "# !pip install Cython\n",
        "# !pip install ConfigSpace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wF5VUaj-Woz-"
      },
      "outputs": [],
      "source": [
        "# !pip install hpbandster ConfigSpace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3BVwwJ3WMlw",
        "outputId": "982a21c3-f75a-481f-af44-5305fee13a8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dragonfly-opt in /usr/local/lib/python3.7/dist-packages (0.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from dragonfly-opt) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from dragonfly-opt) (1.7.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from dragonfly-opt) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from dragonfly-opt) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install dragonfly-opt\n",
        "import dragonfly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFxAXYDNWtM2"
      },
      "outputs": [],
      "source": [
        "from ray.tune.suggest.dragonfly import DragonflySearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pypfAW1pW4u_"
      },
      "outputs": [],
      "source": [
        "import dragonfly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zFCU4ZRl9qZ"
      },
      "outputs": [],
      "source": [
        "## BOHB\n",
        "search_alg = TuneBOHB(\n",
        "    # space=config_space,  # If you want to set the space manually\n",
        "    metric=\"loss\", #or accuracy, etc.\n",
        "    mode=\"min\", #or max\n",
        "    # seed = 42,\n",
        "    # points_to_evaluate=[\n",
        "    # ],\n",
        "    )\n",
        "\n",
        "## Hyperopt\n",
        "search_alg = HyperOptSearch(\n",
        "    # space=config,\n",
        "    metric=\"loss\", #or accuracy, etc.\n",
        "    mode=\"min\", #or max\n",
        "    n_initial_points=60,\n",
        "    # gamma = 0.1,\n",
        "    # random_state_seed=42,\n",
        "    # points_to_evaluate=[{\"max_norm_val\": 2.5, 'GNN_Layers': 3, 'dropout': 0.25, 'dropout1': 0.35, 'dropout2': 0.15, 'lr': 0.001, 'hidden_size': 80, 'readout1_out': 150, 'readout2_out': 100, 'batch_size': 64}],\n",
        "    )\n",
        "\n",
        "## Optuna\n",
        "search_alg = OptunaSearch(\n",
        "    metric=\"loss\", #or accuracy, etc.\n",
        "    mode=\"min\", #or max\n",
        "    # seed = 42,\n",
        "    # points_to_evaluate=[\n",
        "    # {'dropout': 0.2, 'dropout1': 0.1, 'dropout2': 0.25, 'lr': 0.0005, 'hidden_size': 150.0, 'readout1_out': 200.0, 'readout2_out': 180.0, 'max_norm_val': 2.5}\n",
        "    # ],\n",
        "    )\n",
        "\n",
        "# ## Dragonfly\n",
        "# search_alg = DragonflySearch(\n",
        "#     metric=\"loss\", #or accuracy, etc.\n",
        "#     mode=\"min\", #or max\n",
        "#     optimizer=\"bandit\", #[random, bandit, genetic]\n",
        "#     # points_to_evaluate=[\n",
        "#     # {'max_norm_val': 2.5, 'dropout': 0.3, 'dropout1': 0.1, 'dropout2': 0.0, 'lr': 0.0008, 'hidden_size': 90, 'readout1_out': 150, 'readout2_out': 140}\n",
        "#     # ],\n",
        "#     # domain=euclidean, #[cartesian, euclidean]\n",
        "#     )\n",
        "\n",
        "# ## Bayesopt\n",
        "# search_alg = BayesOptSearch(\n",
        "#     metric=\"loss\", #or accuracy, etc.\n",
        "#     mode=\"min\", #or max\n",
        "#     random_search_steps = 60, \n",
        "#     # points_to_evaluate=[\n",
        "#     # {'max_norm_val': 2.5, 'dropout': 0.3, 'dropout1': 0.1, 'dropout2': 0.0, 'lr': 0.0008, 'hidden_size': 90, 'readout1_out': 150, 'readout2_out': 140}\n",
        "#     # ],\n",
        "#     )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bS-JdPltQSdt"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Optimization hyper parameters Esol",
      "provenance": [],
      "authorship_tag": "ABX9TyPg0MASC+OiJvpJbQnIbjgv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}